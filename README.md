# NOVA_TrafficAI

Copy and Paste Information into a chatbot along with all of the files before making any chnages.

üö¶ NOVA 2.0 ‚Äî CONTINUATION & DESIGN CONSTRAINTS README
0. PURPOSE OF THIS DOCUMENT (MANDATORY)

This document exists to lock the system semantics.

If you violate the constraints below, you are no longer working on NOVA 2.0, but on a different system that is not defensible relative to our report, presentation, or design intent.

This is not a greenfield project.

1. WHAT THIS SYSTEM IS (NON-NEGOTIABLE)
Core formulation

This project implements:

Parameter-Shared Multi-Agent Reinforcement Learning (PS-MARL)
with Shared PPO, centralized training, and decentralized execution.

Formally:

Environment: One SUMO traffic network

Agents: N traffic lights (TLS), currently up to 24

Policy: One shared PPO actor‚Äìcritic

Observations: One observation vector per intersection

Actions: One action per intersection per step

Rewards: Per-intersection local reward + shared global reward

This is not:

‚ùå Single-agent PPO

‚ùå One action controlling all lights

‚ùå Independent PPO per intersection

‚ùå Simulator-privileged RL

2. CURRENT IMPLEMENTATION STATUS (WHERE TO PICK UP)
‚úÖ What is already correctly implemented

You are starting from a working architectural baseline:

Environment

MultiIntersectionEnv

Controls N intersections simultaneously

Returns flattened (N √ó obs_dim) observation

Uses *MultiDiscrete([A_max]N) action space

Enforces safe phase switching:

Minimum green time

Phase index clamping

No flickering

Observation pipeline (LOCKED)

Detector-only state

Canonical movement abstraction:

12 buckets = N/S/E/W √ó L/T/R

Per-movement features:

Stop-line occupancy proxy

EWMA queue proxy

Trend

Starvation timer

Control context:

Movement service mask

Time in phase

Eligible-to-change flag

Final obs dim per TLS = 62

‚ö†Ô∏è DO NOT introduce lane waiting time, queue length, or speed from SUMO internals.

Action semantics (LOCKED)

PPO outputs integer a_i ‚àà [0, A_max-1] per TLS

Each TLS has its own valid phase count

Actions are:

Clamped per TLS

Gated by min-green

Safely ignored if invalid

Rewards (CURRENT)

Local (per TLS):

Queue proxy penalty

Starvation penalty

Global:

Mean network queue proxy

Combined:

r_i = Œ± * r_local_i + (1-Œ±) * r_global


Environment returns mean(r_i)

This is acceptable for PPO compatibility.

3. HARD CONSTRAINTS (DO NOT CHANGE)
üö´ Architectural constraints

You must not:

Collapse observations into a single intersection

Output a single action for all intersections

Replace shared PPO with:

Independent PPOs

Centralized monolithic agent

Use simulator-only privileged signals:

Lane waiting time

Queue length

Mean speed

Teleport info

Remove min-green safety logic

Add yellow-phase logic incorrectly (must be explicit, not implicit)

If any of the above happens, stop and revert.

4. WHAT IS INTENTIONALLY ‚ÄúIMPERFECT‚Äù (AND OK FOR NOW)

These are known approximations, not bugs:

Movement L/T/R inference via geometry heuristics

Service inference via detector deltas

Flattened observation for SB3 (instead of native MARL API)

Scalar env reward instead of vector reward

These are acceptable tradeoffs for:

Training stability

Academic defensibility

Real-world deployability

5. IMMEDIATE NEXT STEPS (SAFE TO DO)
üîπ Step 1 ‚Äî Verify training loop stability

Before changing anything else:

Run short training (5‚Äì10k steps)

Confirm:

No TraCI disconnects

No SUMO flag conflicts

No NaNs in observations

PPO updates occur

üîπ Step 2 ‚Äî Reward shaping refinement (SAFE)

You may:

Adjust Œ± schedule (anneal toward global)

Normalize reward magnitudes

Add small phase-change penalty (optional)

Do not add new state inputs.

üîπ Step 3 ‚Äî Logging & diagnostics

Recommended additions:

Per-TLS reward logging

Mean queue proxy over time

Phase switch frequency per TLS

These are read-only diagnostics, not control inputs.

üîπ Step 4 ‚Äî Evaluation harness

Build a separate evaluation script that:

Loads trained model

Runs fixed seeds

Compares against:

Fixed-time baseline

Actuated baseline

Reports:

Delay proxy

Starvation reduction

Stability metrics

6. FUTURE WORK (DO NOT START YET)

These are explicitly postponed:

Yellow phase insertion logic

Curriculum learning

Transfer learning to new networks

True multi-policy MARL frameworks (PettingZoo, RLlib)

Emissions modeling

Real-world controller deployment

7. HOW TO ANSWER ‚ÄúIS THIS REALLY MULTI-AGENT?‚Äù

If asked:

‚ÄúIsn‚Äôt this just one agent?‚Äù

The correct answer is:

This is parameter-shared multi-agent RL.
Each intersection is an agent with its own observation, action, and local reward.
A single shared policy is used for scalability and coordination.

This matches:

Decentralized execution

Centralized training

Modern MARL literature

FHWA-style system thinking

8. FINAL WARNING TO FUTURE AI

If you feel tempted to:

‚ÄúSimplify‚Äù the environment

‚ÄúJust try‚Äù a single action

‚ÄúTemporarily‚Äù use simulator queue length

‚ÄúRefactor‚Äù into one big agent

Stop. You are breaking the project.

Ask instead:

‚ÄúHow do I preserve PS-MARL semantics while improving X?‚Äù

9. TL;DR

Training works ‚Üí do not refactor

Detector-only ‚Üí never violate

One action per TLS per step

Shared PPO ‚â† single agent

Safety logic stays
